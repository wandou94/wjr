{
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "dansbecker_cityscapes_image_pairs_path = kagglehub.dataset_download('dansbecker/cityscapes-image-pairs')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "qhWKDw3tDPo5"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "5ByESw4aDPpB"
      },
      "cell_type": "markdown",
      "source": [
        "#### Dataset:\n",
        "https://www.kaggle.com/dansbecker/cityscapes-image-pairs\n",
        "\n",
        "#### Kaggle Link:\n",
        "https://www.kaggle.com/gokulkarthik/image-segmentation-with-unet-pytorch\n",
        "\n",
        "#### References:\n",
        "* https://arxiv.org/pdf/1603.07285v1.pdf\n",
        "* https://towardsdatascience.com/u-net-b229b32b4a71"
      ]
    },
    {
      "metadata": {
        "id": "174gl4w_DPpD"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "BMpoepgeDPpD"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8fVHMmIDDPpE"
      },
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(device)\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "4ww9cTEzDPpF"
      },
      "cell_type": "code",
      "source": [
        "data_dir = os.path.join(\"/kaggle\", \"input\", \"cityscapes-image-pairs\", \"cityscapes_data\")\n",
        "train_dir = os.path.join(data_dir, \"train\")\n",
        "val_dir = os.path.join(data_dir, \"val\")\n",
        "train_fns = os.listdir(train_dir)\n",
        "val_fns = os.listdir(val_dir)\n",
        "print(len(train_fns), len(val_fns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P2NqspjxDPpF"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Analyze data"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "gjeTP2NUDPpF"
      },
      "cell_type": "code",
      "source": [
        "sample_image_fp = os.path.join(train_dir, train_fns[0])\n",
        "sample_image = Image.open(sample_image_fp).convert(\"RGB\")\n",
        "plt.imshow(sample_image)\n",
        "print(sample_image_fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Dkr-mWXlDPpG"
      },
      "cell_type": "code",
      "source": [
        "def split_image(image):\n",
        "    image = np.array(image)\n",
        "    cityscape, label = image[:, :256, :], image[:, 256:, :]\n",
        "    return cityscape, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Vdf_cd2rDPpG"
      },
      "cell_type": "code",
      "source": [
        "sample_image = np.array(sample_image)\n",
        "print(sample_image.shape)\n",
        "cityscape, label = split_image(sample_image)\n",
        "print(cityscape.min(), cityscape.max(), label.min(), label.max())\n",
        "cityscape, label = Image.fromarray(cityscape), Image.fromarray(label)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "axes[0].imshow(cityscape)\n",
        "axes[1].imshow(label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MLYM4Xu1DPpH"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Define Labels"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "e2HaEP0WDPpH"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "color_set = set()\n",
        "for train_fn in tqdm(train_fns[:10]):\n",
        "    train_fp = os.path.join(train_dir, train_fn)\n",
        "    image = np.array(Image.open(train_fp))\n",
        "    cityscape, label = split_image(sample_image)\n",
        "    label = label.reshape(-1, 3)\n",
        "    local_color_set = set([tuple(c) for c in list(label)])\n",
        "    color_set.update(local_color_set)\n",
        "color_array = np.array(list(color_set))\n",
        "\"\"\"\n",
        "\n",
        "num_items = 1000\n",
        "color_array = np.random.choice(range(256), 3*num_items).reshape(-1, 3)\n",
        "print(color_array.shape)\n",
        "print(color_array[:5, :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "1ryAT1gyDPpI"
      },
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "label_model = KMeans(n_clusters=num_classes)\n",
        "label_model.fit(color_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "zW3n8nu3DPpJ"
      },
      "cell_type": "code",
      "source": [
        "label_model.predict(color_array[:5, :])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "wrLGum78DPpJ"
      },
      "cell_type": "code",
      "source": [
        "cityscape, label = split_image(sample_image)\n",
        "label_class = label_model.predict(label.reshape(-1, 3)).reshape(256, 256)\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "axes[0].imshow(cityscape)\n",
        "axes[1].imshow(label)\n",
        "axes[2].imshow(label_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "VXOiyU-PDPpJ"
      },
      "cell_type": "code",
      "source": [
        "label_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TYowKzenDPpK"
      },
      "cell_type": "markdown",
      "source": [
        "## 4. Define Dataset"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "P-A33RMnDPpK"
      },
      "cell_type": "code",
      "source": [
        "class CityscapeDataset(Dataset):\n",
        "\n",
        "    def __init__(self, image_dir, label_model):\n",
        "        self.image_dir = image_dir\n",
        "        self.image_fns = os.listdir(image_dir)\n",
        "        self.label_model = label_model\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_fns)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image_fn = self.image_fns[index]\n",
        "        image_fp = os.path.join(self.image_dir, image_fn)\n",
        "        image = Image.open(image_fp).convert('RGB')\n",
        "        image = np.array(image)\n",
        "        cityscape, label = self.split_image(image)\n",
        "        label_class = self.label_model.predict(label.reshape(-1, 3)).reshape(256, 256)\n",
        "        cityscape = self.transform(cityscape)\n",
        "        label_class = torch.Tensor(label_class).long()\n",
        "        return cityscape, label_class\n",
        "\n",
        "    def split_image(self, image):\n",
        "        image = np.array(image)\n",
        "        cityscape, label = image[:, :256, :], image[:, 256:, :]\n",
        "        return cityscape, label\n",
        "\n",
        "    def transform(self, image):\n",
        "        transform_ops = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "        ])\n",
        "        return transform_ops(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "3gVg5VIzDPpL"
      },
      "cell_type": "code",
      "source": [
        "dataset = CityscapeDataset(train_dir, label_model)\n",
        "print(len(dataset))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "GJK7y48QDPpL"
      },
      "cell_type": "code",
      "source": [
        "cityscape, label_class = dataset[0]\n",
        "print(cityscape.shape, label_class.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xHjby0QJDPpL"
      },
      "cell_type": "markdown",
      "source": [
        "## 5. Define Model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "xmz7O8YhDPpM"
      },
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.contracting_11 = self.conv_block(in_channels=3, out_channels=64)\n",
        "        self.contracting_12 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.contracting_21 = self.conv_block(in_channels=64, out_channels=128)\n",
        "        self.contracting_22 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.contracting_31 = self.conv_block(in_channels=128, out_channels=256)\n",
        "        self.contracting_32 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.contracting_41 = self.conv_block(in_channels=256, out_channels=512)\n",
        "        self.contracting_42 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.middle = self.conv_block(in_channels=512, out_channels=1024)\n",
        "        self.expansive_11 = nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.expansive_12 = self.conv_block(in_channels=1024, out_channels=512)\n",
        "        self.expansive_21 = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.expansive_22 = self.conv_block(in_channels=512, out_channels=256)\n",
        "        self.expansive_31 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.expansive_32 = self.conv_block(in_channels=256, out_channels=128)\n",
        "        self.expansive_41 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
        "        self.expansive_42 = self.conv_block(in_channels=128, out_channels=64)\n",
        "        self.output = nn.Conv2d(in_channels=64, out_channels=num_classes, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.BatchNorm2d(num_features=out_channels),\n",
        "                                    nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
        "                                    nn.ReLU(),\n",
        "                                    nn.BatchNorm2d(num_features=out_channels))\n",
        "        return block\n",
        "\n",
        "    def forward(self, X):\n",
        "        contracting_11_out = self.contracting_11(X) # [-1, 64, 256, 256]\n",
        "        contracting_12_out = self.contracting_12(contracting_11_out) # [-1, 64, 128, 128]\n",
        "        contracting_21_out = self.contracting_21(contracting_12_out) # [-1, 128, 128, 128]\n",
        "        contracting_22_out = self.contracting_22(contracting_21_out) # [-1, 128, 64, 64]\n",
        "        contracting_31_out = self.contracting_31(contracting_22_out) # [-1, 256, 64, 64]\n",
        "        contracting_32_out = self.contracting_32(contracting_31_out) # [-1, 256, 32, 32]\n",
        "        contracting_41_out = self.contracting_41(contracting_32_out) # [-1, 512, 32, 32]\n",
        "        contracting_42_out = self.contracting_42(contracting_41_out) # [-1, 512, 16, 16]\n",
        "        middle_out = self.middle(contracting_42_out) # [-1, 1024, 16, 16]\n",
        "        expansive_11_out = self.expansive_11(middle_out) # [-1, 512, 32, 32]\n",
        "        expansive_12_out = self.expansive_12(torch.cat((expansive_11_out, contracting_41_out), dim=1)) # [-1, 1024, 32, 32] -> [-1, 512, 32, 32]\n",
        "        expansive_21_out = self.expansive_21(expansive_12_out) # [-1, 256, 64, 64]\n",
        "        expansive_22_out = self.expansive_22(torch.cat((expansive_21_out, contracting_31_out), dim=1)) # [-1, 512, 64, 64] -> [-1, 256, 64, 64]\n",
        "        expansive_31_out = self.expansive_31(expansive_22_out) # [-1, 128, 128, 128]\n",
        "        expansive_32_out = self.expansive_32(torch.cat((expansive_31_out, contracting_21_out), dim=1)) # [-1, 256, 128, 128] -> [-1, 128, 128, 128]\n",
        "        expansive_41_out = self.expansive_41(expansive_32_out) # [-1, 64, 256, 256]\n",
        "        expansive_42_out = self.expansive_42(torch.cat((expansive_41_out, contracting_11_out), dim=1)) # [-1, 128, 256, 256] -> [-1, 64, 256, 256]\n",
        "        output_out = self.output(expansive_42_out) # [-1, num_classes, 256, 256]\n",
        "        return output_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "aFevDJWeDPpN"
      },
      "cell_type": "code",
      "source": [
        "model = UNet(num_classes=num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "vKHSzvZjDPpN"
      },
      "cell_type": "code",
      "source": [
        "data_loader = DataLoader(dataset, batch_size=4)\n",
        "print(len(dataset), len(data_loader))\n",
        "\n",
        "X, Y = iter(data_loader).next()\n",
        "print(X.shape, Y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8a0PKEJJDPpN"
      },
      "cell_type": "code",
      "source": [
        "Y_pred = model(X)\n",
        "print(Y_pred.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bb0NaxVeDPpO"
      },
      "cell_type": "markdown",
      "source": [
        "## 6. Train the model"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Ju-kFXGyDPpO"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "\n",
        "epochs = 10\n",
        "lr = 0.01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "s_KUczl9DPpP"
      },
      "cell_type": "code",
      "source": [
        "dataset = CityscapeDataset(train_dir, label_model)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "RDkVFliaDPpP"
      },
      "cell_type": "code",
      "source": [
        "model = UNet(num_classes=num_classes).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ajGOVYTNDPpP"
      },
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "tRfkydwKDPpQ"
      },
      "cell_type": "code",
      "source": [
        "step_losses = []\n",
        "epoch_losses = []\n",
        "for epoch in tqdm(range(epochs)):\n",
        "    epoch_loss = 0\n",
        "    for X, Y in tqdm(data_loader, total=len(data_loader), leave=False):\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        Y_pred = model(X)\n",
        "        loss = criterion(Y_pred, Y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        step_losses.append(loss.item())\n",
        "    epoch_losses.append(epoch_loss/len(data_loader))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "yd6WDg6XDPpQ"
      },
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "axes[0].plot(step_losses)\n",
        "axes[1].plot(epoch_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "WSpfxPyQDPpR"
      },
      "cell_type": "code",
      "source": [
        "model_name = \"U-Net.pth\"\n",
        "torch.save(model.state_dict(), model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h6HNTwH8DPpS"
      },
      "cell_type": "markdown",
      "source": [
        "## 7. Check model predictions"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-sfR284lDPpS"
      },
      "cell_type": "code",
      "source": [
        "model_path = \"/kaggle/working/U-Net.pth\"\n",
        "model_ = UNet(num_classes=num_classes).to(device)\n",
        "model_.load_state_dict(torch.load(model_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "pIxEel2kDPpT"
      },
      "cell_type": "code",
      "source": [
        "test_batch_size = 8\n",
        "dataset = CityscapeDataset(val_dir, label_model)\n",
        "data_loader = DataLoader(dataset, batch_size=test_batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "79eR34R-DPpT"
      },
      "cell_type": "code",
      "source": [
        "X, Y = next(iter(data_loader))\n",
        "X, Y = X.to(device), Y.to(device)\n",
        "Y_pred = model_(X)\n",
        "print(Y_pred.shape)\n",
        "Y_pred = torch.argmax(Y_pred, dim=1)\n",
        "print(Y_pred.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "DB6K_U-uDPpU"
      },
      "cell_type": "code",
      "source": [
        "inverse_transform = transforms.Compose([\n",
        "    transforms.Normalize((-0.485/0.229, -0.456/0.224, -0.406/0.225), (1/0.229, 1/0.224, 1/0.225))\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "yTQmnPPBDPpV"
      },
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(test_batch_size, 3, figsize=(3*5, test_batch_size*5))\n",
        "\n",
        "for i in range(test_batch_size):\n",
        "\n",
        "    landscape = inverse_transform(X[i]).permute(1, 2, 0).cpu().detach().numpy()\n",
        "    label_class = Y[i].cpu().detach().numpy()\n",
        "    label_class_predicted = Y_pred[i].cpu().detach().numpy()\n",
        "\n",
        "    axes[i, 0].imshow(landscape)\n",
        "    axes[i, 0].set_title(\"Landscape\")\n",
        "    axes[i, 1].imshow(label_class)\n",
        "    axes[i, 1].set_title(\"Label Class\")\n",
        "    axes[i, 2].imshow(label_class_predicted)\n",
        "    axes[i, 2].set_title(\"Label Class - Predicted\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Image-Segmentation-with-UNet-PyTorch",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}