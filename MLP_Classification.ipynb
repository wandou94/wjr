{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"pytorch_multiclass.ipynb\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# 1) Load data\n",
        "df = pd.read_csv('/content/train.csv')\n",
        "\n",
        "# 2) Drop unused columns\n",
        "df = df.drop(columns=['id','CustomerId','Surname'])\n",
        "\n",
        "# 3) Suppose 'Exited' is now a multi‐class label 0…K-1\n",
        "y = df['Exited'].values\n",
        "X = df.drop(columns='Exited')\n",
        "\n",
        "# 4) Encode categoricals\n",
        "le_geo = LabelEncoder().fit(X['Geography'])\n",
        "le_gen = LabelEncoder().fit(X['Gender'])\n",
        "X['Geography'] = le_geo.transform(X['Geography'])\n",
        "X['Gender']    = le_gen.transform(X['Gender'])\n",
        "\n",
        "# 5) Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X.values, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 6) Standardize\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "# 7) Build PyTorch datasets\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "X_train_t = torch.from_numpy(X_train).float().to(device)\n",
        "y_train_t = torch.from_numpy(y_train).long().to(device)       # note .long()\n",
        "X_test_t  = torch.from_numpy(X_test).float().to(device)\n",
        "y_test_t  = torch.from_numpy(y_test).long().to(device)\n",
        "\n",
        "train_ds = TensorDataset(X_train_t, y_train_t)\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "\n",
        "# 8) Determine number of classes\n",
        "num_classes = len(np.unique(y_train))\n",
        "\n",
        "# 9) Define MLP\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 64), nn.ReLU(), nn.Dropout(0.3),\n",
        "            nn.Linear(64, 32),    nn.ReLU(), nn.Dropout(0.3),\n",
        "            nn.Linear(32, out_dim)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = MLP(X_train.shape[1], num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "# 10) Training loop\n",
        "epochs = 200\n",
        "for epoch in range(1, epochs+1):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for xb, yb in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)                   # shape [B, num_classes]\n",
        "        loss   = criterion(logits, yb)       # expects yb long [B]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * xb.size(0)\n",
        "    avg_loss = running_loss / len(train_ds)\n",
        "    print(f\"Epoch {epoch}/{epochs}  Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# 11) Evaluate on test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(X_test_t)\n",
        "    preds = logits.argmax(dim=1).cpu().numpy()\n",
        "    y_true = y_test\n",
        "\n",
        "print(\"\\nClassification Report on Test Set:\")\n",
        "print(classification_report(y_true, preds, digits=4))\n",
        "\n",
        "# 12) Inference on unseen test.csv\n",
        "test_df = pd.read_csv('/content/test.csv')\n",
        "ids = test_df['id']\n",
        "test_df = test_df.drop(columns=['CustomerId','Surname'])\n",
        "test_df['Geography'] = le_geo.transform(test_df['Geography'])\n",
        "test_df['Gender']    = le_gen.transform(test_df['Gender'])\n",
        "X_sub = scaler.transform(test_df.drop(columns='id').values)\n",
        "X_sub_t = torch.from_numpy(X_sub).float().to(device)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(X_sub_t)\n",
        "    preds  = logits.argmax(dim=1).cpu().numpy()\n",
        "\n",
        "submission = pd.DataFrame({'id': ids, 'Exited': preds})\n",
        "submission.to_csv('prediction.csv', index=False)\n",
        "print(\"Saved prediction.csv\")"
      ],
      "metadata": {
        "id": "G-dp27UrVWzZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cbbca5e-8dfe-4a27-8637-2212e3ca5354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200  Loss: 0.6744\n",
            "Epoch 2/200  Loss: 0.6251\n",
            "Epoch 3/200  Loss: 0.5880\n",
            "Epoch 4/200  Loss: 0.5620\n",
            "Epoch 5/200  Loss: 0.5417\n",
            "Epoch 6/200  Loss: 0.5265\n",
            "Epoch 7/200  Loss: 0.5162\n",
            "Epoch 8/200  Loss: 0.5069\n",
            "Epoch 9/200  Loss: 0.5024\n",
            "Epoch 10/200  Loss: 0.4945\n",
            "Epoch 11/200  Loss: 0.4909\n",
            "Epoch 12/200  Loss: 0.4885\n",
            "Epoch 13/200  Loss: 0.4822\n",
            "Epoch 14/200  Loss: 0.4797\n",
            "Epoch 15/200  Loss: 0.4740\n",
            "Epoch 16/200  Loss: 0.4732\n",
            "Epoch 17/200  Loss: 0.4690\n",
            "Epoch 18/200  Loss: 0.4670\n",
            "Epoch 19/200  Loss: 0.4616\n",
            "Epoch 20/200  Loss: 0.4590\n",
            "Epoch 21/200  Loss: 0.4574\n",
            "Epoch 22/200  Loss: 0.4526\n",
            "Epoch 23/200  Loss: 0.4495\n",
            "Epoch 24/200  Loss: 0.4475\n",
            "Epoch 25/200  Loss: 0.4483\n",
            "Epoch 26/200  Loss: 0.4436\n",
            "Epoch 27/200  Loss: 0.4439\n",
            "Epoch 28/200  Loss: 0.4400\n",
            "Epoch 29/200  Loss: 0.4352\n",
            "Epoch 30/200  Loss: 0.4351\n",
            "Epoch 31/200  Loss: 0.4327\n",
            "Epoch 32/200  Loss: 0.4297\n",
            "Epoch 33/200  Loss: 0.4269\n",
            "Epoch 34/200  Loss: 0.4233\n",
            "Epoch 35/200  Loss: 0.4214\n",
            "Epoch 36/200  Loss: 0.4202\n",
            "Epoch 37/200  Loss: 0.4197\n",
            "Epoch 38/200  Loss: 0.4186\n",
            "Epoch 39/200  Loss: 0.4165\n",
            "Epoch 40/200  Loss: 0.4133\n",
            "Epoch 41/200  Loss: 0.4122\n",
            "Epoch 42/200  Loss: 0.4093\n",
            "Epoch 43/200  Loss: 0.4068\n",
            "Epoch 44/200  Loss: 0.4048\n",
            "Epoch 45/200  Loss: 0.4027\n",
            "Epoch 46/200  Loss: 0.4019\n",
            "Epoch 47/200  Loss: 0.4003\n",
            "Epoch 48/200  Loss: 0.3992\n",
            "Epoch 49/200  Loss: 0.3954\n",
            "Epoch 50/200  Loss: 0.3968\n",
            "Epoch 51/200  Loss: 0.3978\n",
            "Epoch 52/200  Loss: 0.3953\n",
            "Epoch 53/200  Loss: 0.3958\n",
            "Epoch 54/200  Loss: 0.3938\n",
            "Epoch 55/200  Loss: 0.3939\n",
            "Epoch 56/200  Loss: 0.3886\n",
            "Epoch 57/200  Loss: 0.3868\n",
            "Epoch 58/200  Loss: 0.3891\n",
            "Epoch 59/200  Loss: 0.3873\n",
            "Epoch 60/200  Loss: 0.3843\n",
            "Epoch 61/200  Loss: 0.3871\n",
            "Epoch 62/200  Loss: 0.3853\n",
            "Epoch 63/200  Loss: 0.3815\n",
            "Epoch 64/200  Loss: 0.3820\n",
            "Epoch 65/200  Loss: 0.3859\n",
            "Epoch 66/200  Loss: 0.3829\n",
            "Epoch 67/200  Loss: 0.3803\n",
            "Epoch 68/200  Loss: 0.3800\n",
            "Epoch 69/200  Loss: 0.3805\n",
            "Epoch 70/200  Loss: 0.3794\n",
            "Epoch 71/200  Loss: 0.3787\n",
            "Epoch 72/200  Loss: 0.3768\n",
            "Epoch 73/200  Loss: 0.3765\n",
            "Epoch 74/200  Loss: 0.3757\n",
            "Epoch 75/200  Loss: 0.3750\n",
            "Epoch 76/200  Loss: 0.3753\n",
            "Epoch 77/200  Loss: 0.3777\n",
            "Epoch 78/200  Loss: 0.3749\n",
            "Epoch 79/200  Loss: 0.3770\n",
            "Epoch 80/200  Loss: 0.3748\n",
            "Epoch 81/200  Loss: 0.3712\n",
            "Epoch 82/200  Loss: 0.3679\n",
            "Epoch 83/200  Loss: 0.3691\n",
            "Epoch 84/200  Loss: 0.3732\n",
            "Epoch 85/200  Loss: 0.3672\n",
            "Epoch 86/200  Loss: 0.3714\n",
            "Epoch 87/200  Loss: 0.3691\n",
            "Epoch 88/200  Loss: 0.3678\n",
            "Epoch 89/200  Loss: 0.3696\n",
            "Epoch 90/200  Loss: 0.3671\n",
            "Epoch 91/200  Loss: 0.3686\n",
            "Epoch 92/200  Loss: 0.3651\n",
            "Epoch 93/200  Loss: 0.3681\n",
            "Epoch 94/200  Loss: 0.3658\n",
            "Epoch 95/200  Loss: 0.3656\n",
            "Epoch 96/200  Loss: 0.3642\n",
            "Epoch 97/200  Loss: 0.3681\n",
            "Epoch 98/200  Loss: 0.3657\n",
            "Epoch 99/200  Loss: 0.3606\n",
            "Epoch 100/200  Loss: 0.3624\n",
            "Epoch 101/200  Loss: 0.3660\n",
            "Epoch 102/200  Loss: 0.3616\n",
            "Epoch 103/200  Loss: 0.3630\n",
            "Epoch 104/200  Loss: 0.3633\n",
            "Epoch 105/200  Loss: 0.3598\n",
            "Epoch 106/200  Loss: 0.3611\n",
            "Epoch 107/200  Loss: 0.3615\n",
            "Epoch 108/200  Loss: 0.3599\n",
            "Epoch 109/200  Loss: 0.3618\n",
            "Epoch 110/200  Loss: 0.3615\n",
            "Epoch 111/200  Loss: 0.3607\n",
            "Epoch 112/200  Loss: 0.3588\n",
            "Epoch 113/200  Loss: 0.3594\n",
            "Epoch 114/200  Loss: 0.3586\n",
            "Epoch 115/200  Loss: 0.3567\n",
            "Epoch 116/200  Loss: 0.3542\n",
            "Epoch 117/200  Loss: 0.3586\n",
            "Epoch 118/200  Loss: 0.3608\n",
            "Epoch 119/200  Loss: 0.3566\n",
            "Epoch 120/200  Loss: 0.3596\n",
            "Epoch 121/200  Loss: 0.3553\n",
            "Epoch 122/200  Loss: 0.3547\n",
            "Epoch 123/200  Loss: 0.3550\n",
            "Epoch 124/200  Loss: 0.3559\n",
            "Epoch 125/200  Loss: 0.3547\n",
            "Epoch 126/200  Loss: 0.3543\n",
            "Epoch 127/200  Loss: 0.3541\n",
            "Epoch 128/200  Loss: 0.3558\n",
            "Epoch 129/200  Loss: 0.3524\n",
            "Epoch 130/200  Loss: 0.3536\n",
            "Epoch 131/200  Loss: 0.3511\n",
            "Epoch 132/200  Loss: 0.3535\n",
            "Epoch 133/200  Loss: 0.3513\n",
            "Epoch 134/200  Loss: 0.3523\n",
            "Epoch 135/200  Loss: 0.3499\n",
            "Epoch 136/200  Loss: 0.3530\n",
            "Epoch 137/200  Loss: 0.3558\n",
            "Epoch 138/200  Loss: 0.3491\n",
            "Epoch 139/200  Loss: 0.3525\n",
            "Epoch 140/200  Loss: 0.3467\n",
            "Epoch 141/200  Loss: 0.3489\n",
            "Epoch 142/200  Loss: 0.3508\n",
            "Epoch 143/200  Loss: 0.3483\n",
            "Epoch 144/200  Loss: 0.3496\n",
            "Epoch 145/200  Loss: 0.3501\n",
            "Epoch 146/200  Loss: 0.3495\n",
            "Epoch 147/200  Loss: 0.3469\n",
            "Epoch 148/200  Loss: 0.3472\n",
            "Epoch 149/200  Loss: 0.3478\n",
            "Epoch 150/200  Loss: 0.3511\n",
            "Epoch 151/200  Loss: 0.3464\n",
            "Epoch 152/200  Loss: 0.3493\n",
            "Epoch 153/200  Loss: 0.3482\n",
            "Epoch 154/200  Loss: 0.3478\n",
            "Epoch 155/200  Loss: 0.3452\n",
            "Epoch 156/200  Loss: 0.3484\n",
            "Epoch 157/200  Loss: 0.3452\n",
            "Epoch 158/200  Loss: 0.3463\n",
            "Epoch 159/200  Loss: 0.3478\n",
            "Epoch 160/200  Loss: 0.3458\n",
            "Epoch 161/200  Loss: 0.3450\n",
            "Epoch 162/200  Loss: 0.3443\n",
            "Epoch 163/200  Loss: 0.3460\n",
            "Epoch 164/200  Loss: 0.3480\n",
            "Epoch 165/200  Loss: 0.3458\n",
            "Epoch 166/200  Loss: 0.3494\n",
            "Epoch 167/200  Loss: 0.3446\n",
            "Epoch 168/200  Loss: 0.3460\n",
            "Epoch 169/200  Loss: 0.3426\n",
            "Epoch 170/200  Loss: 0.3416\n",
            "Epoch 171/200  Loss: 0.3401\n",
            "Epoch 172/200  Loss: 0.3426\n",
            "Epoch 173/200  Loss: 0.3454\n",
            "Epoch 174/200  Loss: 0.3447\n",
            "Epoch 175/200  Loss: 0.3430\n",
            "Epoch 176/200  Loss: 0.3401\n",
            "Epoch 177/200  Loss: 0.3418\n",
            "Epoch 178/200  Loss: 0.3469\n",
            "Epoch 179/200  Loss: 0.3438\n",
            "Epoch 180/200  Loss: 0.3399\n",
            "Epoch 181/200  Loss: 0.3387\n",
            "Epoch 182/200  Loss: 0.3404\n",
            "Epoch 183/200  Loss: 0.3390\n",
            "Epoch 184/200  Loss: 0.3388\n",
            "Epoch 185/200  Loss: 0.3401\n",
            "Epoch 186/200  Loss: 0.3402\n",
            "Epoch 187/200  Loss: 0.3413\n",
            "Epoch 188/200  Loss: 0.3427\n",
            "Epoch 189/200  Loss: 0.3431\n",
            "Epoch 190/200  Loss: 0.3408\n",
            "Epoch 191/200  Loss: 0.3382\n",
            "Epoch 192/200  Loss: 0.3394\n",
            "Epoch 193/200  Loss: 0.3407\n",
            "Epoch 194/200  Loss: 0.3419\n",
            "Epoch 195/200  Loss: 0.3389\n",
            "Epoch 196/200  Loss: 0.3411\n",
            "Epoch 197/200  Loss: 0.3363\n",
            "Epoch 198/200  Loss: 0.3374\n",
            "Epoch 199/200  Loss: 0.3340\n",
            "Epoch 200/200  Loss: 0.3406\n",
            "\n",
            "Classification Report on Test Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.8933    0.9509    0.9212      2404\n",
            "         1.0     0.7324    0.5419    0.6230       596\n",
            "\n",
            "    accuracy                         0.8697      3000\n",
            "   macro avg     0.8129    0.7464    0.7721      3000\n",
            "weighted avg     0.8614    0.8697    0.8620      3000\n",
            "\n",
            "Saved prediction.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WputN_YxaMpi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
