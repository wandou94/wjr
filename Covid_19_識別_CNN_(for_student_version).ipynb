{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 1117472,
          "sourceType": "datasetVersion",
          "datasetId": 627146
        }
      ],
      "dockerImageVersionId": 30761,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch CNN Covid19"
      ],
      "metadata": {
        "id": "0wVsz2IpPKRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-36pNcKNlk6",
        "outputId": "a554a618-bff4-4a53-9e76-d7b38b5202a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cp /content/drive/MyDrive/archive.zip ."
      ],
      "metadata": {
        "id": "bi_FH2oTCh5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# !pip install torchvision\n",
        "import torchvision\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms"
      ],
      "metadata": {
        "id": "rVPVpPDLCO4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip archive.zip"
      ],
      "metadata": {
        "id": "NfhRjh1d6RT6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac50ab8a-dc19-4b3b-dd4d-767bdfb4be25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  archive.zip\n",
            "  inflating: Covid19-dataset/test/Covid/0100.jpeg  \n",
            "  inflating: Covid19-dataset/test/Covid/0102.jpeg  \n",
            "  inflating: Covid19-dataset/test/Covid/0105.png  \n",
            "  inflating: Covid19-dataset/test/Covid/0106.jpeg  \n",
            "  inflating: Covid19-dataset/test/Covid/0108.jpeg  \n",
            "  inflating: Covid19-dataset/test/Covid/0111.jpg  \n",
            "  inflating: Covid19-dataset/test/Covid/0112.jpg  \n",
            "  inflating: Covid19-dataset/test/Covid/0113.jpg  \n",
            "  inflating: Covid19-dataset/test/Covid/0115.jpeg  \n",
            "  inflating: Covid19-dataset/test/Covid/0118.jpeg  \n",
            "  inflating: Covid19-dataset/test/Covid/0119.jpeg  \n",
            "  inflating: Covid19-dataset/test/Covid/0120.jpg  \n",
            "  inflating: Covid19-dataset/test/Covid/094.png  \n",
            "  inflating: Covid19-dataset/test/Covid/096.png  \n",
            "  inflating: Covid19-dataset/test/Covid/098.jpeg  \n",
            "  inflating: Covid19-dataset/test/Covid/COVID-00003b.jpg  \n",
            "  inflating: Covid19-dataset/test/Covid/COVID-00012.jpg  \n",
            "  inflating: Covid19-dataset/test/Covid/COVID-00022.jpg  \n",
            "  inflating: Covid19-dataset/test/Covid/COVID-00033.jpg  \n",
            "  inflating: Covid19-dataset/test/Covid/COVID-00037.jpg  \n",
            "  inflating: Covid19-dataset/test/Covid/auntminnie-2020_01_31_20_24_2322_2020_01_31_x-ray_coronavirus_US.jpg  \n",
            "  inflating: Covid19-dataset/test/Covid/auntminnie-a-2020_01_28_23_51_6665_2020_01_28_Vietnam_coronavirus.jpeg  \n",
            "  inflating: Covid19-dataset/test/Covid/auntminnie-b-2020_01_28_23_51_6665_2020_01_28_Vietnam_coronavirus.jpeg  \n",
            "  inflating: Covid19-dataset/test/Covid/auntminnie-c-2020_01_28_23_51_6665_2020_01_28_Vietnam_coronavirus.jpeg  \n",
            "  inflating: Covid19-dataset/test/Covid/auntminnie-d-2020_01_28_23_51_6665_2020_01_28_Vietnam_coronavirus.jpeg  \n",
            "  inflating: Covid19-dataset/test/Covid/radiopaedia-2019-novel-coronavirus-infected-pneumonia.jpg  \n",
            "  inflating: Covid19-dataset/test/Normal/0101.jpeg  \n",
            "  inflating: Covid19-dataset/test/Normal/0102.jpeg  \n",
            "  inflating: Covid19-dataset/test/Normal/0103.jpeg  \n",
            "  inflating: Covid19-dataset/test/Normal/0105.jpeg  \n",
            "  inflating: Covid19-dataset/test/Normal/0106.jpeg  \n",
            "  inflating: Covid19-dataset/test/Normal/0107.jpeg  \n",
            "  inflating: Covid19-dataset/test/Normal/0108.jpeg  \n",
            "  inflating: Covid19-dataset/test/Normal/0109.jpeg  \n",
            "  inflating: Covid19-dataset/test/Normal/0110.jpeg  \n",
            "  inflating: Covid19-dataset/test/Normal/0111.jpeg  \n",
            "  inflating: Covid19-dataset/test/Normal/0112.jpeg  \n",
            "  inflating: Covid19-dataset/test/Normal/0114.jpeg  \n",
            "  inflating: Covid19-dataset/test/Normal/0115.jpeg  \n",
            "  inflating: Covid19-dataset/test/Normal/0116.jpeg  \n",
            "  inflating: Covid19-dataset/test/Normal/0117.jpeg  \n",
            "  inflating: Covid19-dataset/test/Normal/0118.jpeg  \n",
            "  inflating: Covid19-dataset/test/Normal/0119.jpeg  \n",
            "  inflating: Covid19-dataset/test/Normal/0120.jpeg  \n",
            "  inflating: Covid19-dataset/test/Normal/0121.jpeg  \n",
            "  inflating: Covid19-dataset/test/Normal/0122.jpeg  \n",
            "  inflating: Covid19-dataset/test/Viral Pneumonia/0101.jpeg  \n",
            "  inflating: Covid19-dataset/test/Viral Pneumonia/0102.jpeg  \n",
            "  inflating: Covid19-dataset/test/Viral Pneumonia/0103.jpeg  \n",
            "  inflating: Covid19-dataset/test/Viral Pneumonia/0104.jpeg  \n",
            "  inflating: Covid19-dataset/test/Viral Pneumonia/0105.jpeg  \n",
            "  inflating: Covid19-dataset/test/Viral Pneumonia/0106.jpeg  \n",
            "  inflating: Covid19-dataset/test/Viral Pneumonia/0107.jpeg  \n",
            "  inflating: Covid19-dataset/test/Viral Pneumonia/0108.jpeg  \n",
            "  inflating: Covid19-dataset/test/Viral Pneumonia/0109.jpeg  \n",
            "  inflating: Covid19-dataset/test/Viral Pneumonia/0110.jpeg  \n",
            "  inflating: Covid19-dataset/test/Viral Pneumonia/0111.jpeg  \n",
            "  inflating: Covid19-dataset/test/Viral Pneumonia/0112.jpeg  \n",
            "  inflating: Covid19-dataset/test/Viral Pneumonia/0113.jpeg  \n",
            "  inflating: Covid19-dataset/test/Viral Pneumonia/0114.jpeg  \n",
            "  inflating: Covid19-dataset/test/Viral Pneumonia/0115.jpeg  \n",
            "  inflating: Covid19-dataset/test/Viral Pneumonia/0116.jpeg  \n",
            "  inflating: Covid19-dataset/test/Viral Pneumonia/0117.jpeg  \n",
            "  inflating: Covid19-dataset/test/Viral Pneumonia/0118.jpeg  \n",
            "  inflating: Covid19-dataset/test/Viral Pneumonia/0119.jpeg  \n",
            "  inflating: Covid19-dataset/test/Viral Pneumonia/0120.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/01.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/010.png  \n",
            "  inflating: Covid19-dataset/train/Covid/012.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/015.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/019.png  \n",
            "  inflating: Covid19-dataset/train/Covid/02.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/020.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/021.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/022.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/024.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/025.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/026.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/027.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/03.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/031.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/032.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/033.png  \n",
            "  inflating: Covid19-dataset/train/Covid/039.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/04.png  \n",
            "  inflating: Covid19-dataset/train/Covid/040.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/041.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/042.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/043.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/044.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/045.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/046.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/047.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/048.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/049.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/050.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/051.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/052.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/053.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/054.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/055.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/056.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/057.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/058.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/059.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/06.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/060.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/061.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/062.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/064.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/065.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/067.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/068.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/069.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/07.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/071.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/072.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/073.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/074.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/076.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/078.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/079.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/08.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/080.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/081.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/082.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/083.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/084.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/085.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/086.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/088.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/089.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/09.png  \n",
            "  inflating: Covid19-dataset/train/Covid/090.jpeg  \n",
            "  inflating: Covid19-dataset/train/Covid/091.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/092.png  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00001.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00002.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00003a.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00003b.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00004.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00005.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00006.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00007.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00008.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00009.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00010.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00011.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00012.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00013a.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00013b.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00014.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00015a.png  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00015b.png  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00016.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00017.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00018.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00019.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00020.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00021.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00022.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00023.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00024.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00025.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00026.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00027.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00028.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00029.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00030.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00031.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00032.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00033.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00034.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00035.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00036.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00037.jpg  \n",
            "  inflating: Covid19-dataset/train/Covid/COVID-00038.jpg  \n",
            "  inflating: Covid19-dataset/train/Normal/01.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/010.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/011.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/012.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/013.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/014.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/015.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/016.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/017.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/018.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/019.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/02.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/020.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/021.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/022.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/023.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/024.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/025.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/03.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/04.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/05.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/050.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/051.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/052.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/053.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/054.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/055.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/056.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/057.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/058.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/059.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/06.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/060.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/061.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/062.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/063.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/064.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/065.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/066.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/067.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/068.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/069.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/07.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/070.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/071.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/072.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/073.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/074.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/075.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/076.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/077.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/079.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/08.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/080.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/081.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/082.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/083.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/084.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/085.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/086.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/087.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/088.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/09.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/091.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/092.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/093.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/094.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/095.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/096.jpeg  \n",
            "  inflating: Covid19-dataset/train/Normal/097.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/01.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/010.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/011.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/012.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/013.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/016.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/018.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/019.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/02.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/020.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/021.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/022.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/023.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/024.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/025.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/027.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/03.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/031.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/032.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/033.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/034.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/035.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/036.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/037.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/038.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/04.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/041.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/042.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/043.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/044.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/045.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/046.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/047.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/048.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/05.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/051.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/052.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/053.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/054.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/055.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/056.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/057.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/058.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/06.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/061.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/062.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/063.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/064.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/065.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/066.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/067.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/068.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/07.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/071.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/072.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/073.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/074.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/075.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/076.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/077.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/078.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/08.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/081.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/082.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/083.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/084.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/09.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/094.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/095.jpeg  \n",
            "  inflating: Covid19-dataset/train/Viral Pneumonia/096.jpeg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "data_transforms = {\n",
        "    'train':\n",
        "    transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]),\n",
        "    'validation':\n",
        "    transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]),\n",
        "}\n",
        "\n",
        "image_datasets = {\n",
        "    'train':\n",
        "    datasets.ImageFolder('/content/Covid19-dataset/train', data_transforms['train']),\n",
        "    'validation':\n",
        "    datasets.ImageFolder( '/content/Covid19-dataset/test', data_transforms['validation'])\n",
        "}\n",
        "\n",
        "dataloaders = {\n",
        "    'train':\n",
        "    torch.utils.data.DataLoader(image_datasets['train'],\n",
        "                                batch_size=784,\n",
        "                                shuffle=True,\n",
        "                                num_workers=0),\n",
        "    'validation':\n",
        "    torch.utils.data.DataLoader(image_datasets['validation'],\n",
        "                                batch_size=32,\n",
        "                                shuffle=False,\n",
        "                                num_workers=0)\n",
        "}"
      ],
      "metadata": {
        "id": "HHtH2mV77pmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "   def __init__(self, in_channels, num_classes):\n",
        "\n",
        "       super(CNN, self).__init__()\n",
        "\n",
        "       # 1st convolutional layer\n",
        "       self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=3, padding=1)\n",
        "       # Max pooling layer\n",
        "       self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "       # 2nd convolutional layer\n",
        "       self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\n",
        "       # Fully connected layer\n",
        "       self.fc1 = nn.Linear(16 * 56 * 56, 3)\n",
        "\n",
        "\n",
        "   def forward(self, x):\n",
        "       x = F.relu(self.conv1(x))\n",
        "       x = self.pool(x)\n",
        "       x = F.relu(self.conv2(x))\n",
        "       x = self.pool(x)\n",
        "       x = x.reshape(x.shape[0], -1)\n",
        "       x = self.fc1(x)\n",
        "       return x"
      ],
      "metadata": {
        "id": "nEBWOk2D9eD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class CNN(nn.Module):\n",
        "#     def __init__(self, in_channels, num_classes):\n",
        "#         super(CNN, self).__init__()\n",
        "\n",
        "#         # 1st convolutional layer: output channels = 8\n",
        "#         self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=3, padding=1)\n",
        "\n",
        "#         # Max pooling layer\n",
        "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "#         # 2nd convolutional layer: output channels = 16\n",
        "#         self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\n",
        "\n",
        "#         # Fully connected layers:\n",
        "#         # Assuming input images of size 224x224,\n",
        "#         # after two poolings, the spatial dimensions become 56x56.\n",
        "#         # Therefore, the flattened dimension is 16 * 56 * 56.\n",
        "#         self.fc1 = nn.Linear(16 * 56 * 56, 4096)\n",
        "#         self.fc2 = nn.Linear(4096, 1024)\n",
        "#         self.fc3 = nn.Linear(1024, num_classes)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Convolutional block 1\n",
        "#         x = F.relu(self.conv1(x))\n",
        "#         x = self.pool(x)\n",
        "\n",
        "#         # Convolutional block 2\n",
        "#         x = F.relu(self.conv2(x))\n",
        "#         x = self.pool(x)\n",
        "\n",
        "#         # Flatten the tensor (maintaining the batch dimension)\n",
        "#         x = x.reshape(x.shape[0], -1)\n",
        "\n",
        "#         # Fully connected layers with ReLU activations in between\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = F.relu(self.fc2(x))\n",
        "#         x = self.fc3(x)  # Final output (logits)\n",
        "\n",
        "#         return x\n"
      ],
      "metadata": {
        "id": "Av6mMViIf7zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "   def __init__(self, in_channels, num_classes):\n",
        "\n",
        "       super(CNN, self).__init__()\n",
        "\n",
        "       # 1st convolutional layer\n",
        "       self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=8, kernel_size=3, padding=1)\n",
        "       # Max pooling layer\n",
        "       self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "       # 2nd convolutional layer\n",
        "       self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1)\n",
        "       # Fully connected layer\n",
        "       self.fc1 = nn.Linear(16 * 56 * 56, 3)\n",
        "\n",
        "\n",
        "   def forward(self, x):\n",
        "       x = F.relu(self.conv1(x))\n",
        "       x = self.pool(x)\n",
        "       x = F.relu(self.conv2(x))\n",
        "       x = self.pool(x)\n",
        "       x = x.reshape(x.shape[0], -1)\n",
        "       x = self.fc1(x)\n",
        "       return x"
      ],
      "metadata": {
        "id": "tc0O7YpafurF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = CNN(in_channels=3, num_classes=3).to(device)\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "8xtKgYsL9hRo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f74cfc4-3d3b-4a54-b2d3-22580146190e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (conv1): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (fc1): Linear(in_features=50176, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "daHQ0z0j9m2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=10\n",
        "for epoch in range(num_epochs):\n",
        " # Iterate over training batches\n",
        "   print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
        "\n",
        "   for batch_index, (data, targets) in enumerate(tqdm(dataloaders['train'])):\n",
        "       data = data.to(device)\n",
        "       targets = targets.to(device)\n",
        "       scores = model(data)\n",
        "       loss = criterion(scores, targets)\n",
        "       optimizer.zero_grad()\n",
        "       loss.backward()\n",
        "       optimizer.step()"
      ],
      "metadata": {
        "id": "5L4h38hC-L6U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe252b7e-4d88-4c34-bf5a-85401650c1c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.99s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.98s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.91s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:09<00:00,  9.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:09<00:00,  9.25s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:08<00:00,  8.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:09<00:00,  9.35s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:09<00:00,  9.30s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "model.eval()\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, targets in dataloaders['validation']:\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "        outputs = model(data)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        y_true.extend(targets.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "precision = precision_score(y_true, y_pred, average='weighted')\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "print(\"Test Precision: {:.4f}\".format(precision))\n",
        "print(\"Test Recall:    {:.4f}\".format(recall))\n",
        "print(\"Test F1-score:  {:.4f}\".format(f1))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHs-R69SPZqN",
        "outputId": "2b30e08c-eeb7-4fa8-ff56-8f9daebc7978"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Precision: 0.4716\n",
            "Test Recall:    0.6515\n",
            "Test F1-score:  0.5440\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      1.00      0.90        26\n",
            "           1       0.00      0.00      0.00        20\n",
            "           2       0.50      0.85      0.63        20\n",
            "\n",
            "    accuracy                           0.65        66\n",
            "   macro avg       0.44      0.62      0.51        66\n",
            "weighted avg       0.47      0.65      0.54        66\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iQWtdKiePbtZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}